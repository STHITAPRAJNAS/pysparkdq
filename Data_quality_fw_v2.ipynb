{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e28381-dcba-4bd3-aee4-f55aff72c369",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33edbbb6-7238-463f-bd62-b2992ad43ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/22 13:31:33 WARN Utils: Your hostname, Sthitaprajnas-MacBook-M1Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.61 instead (on interface en0)\n",
      "24/06/22 13:31:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/pappsmac/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pappsmac/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-136fa463-533d-401d-b513-b280bd375853;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pappsmac/opt/anaconda3/envs/synthetic/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (165ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (40ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (38ms)\n",
      ":: resolution report :: resolve 907ms :: artifacts dl 283ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-136fa463-533d-401d-b513-b280bd375853\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (3759kB/7ms)\n",
      "24/06/22 13:31:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/22 13:31:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "from typing import List, Dict, Union\n",
    "import concurrent.futures\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "# Set environment variables\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk-21.jdk/Contents/Home'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local pyspark-shell'\n",
    "\n",
    "# Initialize findspark\n",
    "findspark.init()\n",
    "#spark.stop()\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenericDataGenerationExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f998c6e9-74a7-4916-945c-a9b712681012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n",
      "+---+---+--------+----------+\n",
      "| id|age|  salary|department|\n",
      "+---+---+--------+----------+\n",
      "|  1| 33| 98833.5|        HR|\n",
      "|  2| 33| 66562.5|        HR|\n",
      "|  3| 53| 53133.0|        HR|\n",
      "|  4| 50| 42029.0|     Sales|\n",
      "|  5| 44| 32924.0| Marketing|\n",
      "|  6| 59| 71259.0|   Finance|\n",
      "|  7| 22| 35696.0|   Finance|\n",
      "|  8| 18| 51879.0|     Sales|\n",
      "|  9| 29| 62150.0|        HR|\n",
      "| 10| 44| 64152.0|   Finance|\n",
      "| 11| 50| 64304.0| Marketing|\n",
      "| 12| 22| 43051.0|     Sales|\n",
      "| 13| 39| 66033.0|     Sales|\n",
      "| 14| 43| 41702.0|   Finance|\n",
      "| 15| 18| 57894.0|   Finance|\n",
      "| 16| 22| 62746.0|   Finance|\n",
      "| 17| 56| 57478.0|   Finance|\n",
      "| 18| 38| 77784.0|        IT|\n",
      "| 19| 60| 59880.0| Marketing|\n",
      "| 20| 52|132763.5| Marketing|\n",
      "+---+---+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame saved as table: employees2\n",
      "\n",
      "DataFrame read from table:\n",
      "+---+---+--------+----------+\n",
      "| id|age|  salary|department|\n",
      "+---+---+--------+----------+\n",
      "|  1| 33| 98833.5|        HR|\n",
      "|  2| 33| 66562.5|        HR|\n",
      "|  3| 53| 53133.0|        HR|\n",
      "|  4| 50| 42029.0|     Sales|\n",
      "|  5| 44| 32924.0| Marketing|\n",
      "|  6| 59| 71259.0|   Finance|\n",
      "|  7| 22| 35696.0|   Finance|\n",
      "|  8| 18| 51879.0|     Sales|\n",
      "|  9| 29| 62150.0|        HR|\n",
      "| 10| 44| 64152.0|   Finance|\n",
      "| 11| 50| 64304.0| Marketing|\n",
      "| 12| 22| 43051.0|     Sales|\n",
      "| 13| 39| 66033.0|     Sales|\n",
      "| 14| 43| 41702.0|   Finance|\n",
      "| 15| 18| 57894.0|   Finance|\n",
      "| 16| 22| 62746.0|   Finance|\n",
      "| 17| 56| 57478.0|   Finance|\n",
      "| 18| 38| 77784.0|        IT|\n",
      "| 19| 60| 59880.0| Marketing|\n",
      "| 20| 52|132763.5| Marketing|\n",
      "+---+---+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, when, lit\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Create a sample DataFrame\n",
    "def create_sample_dataframe(num_rows=30):\n",
    "    data = [(\n",
    "        i,  # id\n",
    "        random.randint(20, 60),  # age\n",
    "        random.randint(30000, 100000),  # salary\n",
    "        random.choice(['IT', 'HR', 'Finance', 'Marketing', 'Sales'])  # department\n",
    "    ) for i in range(1, num_rows + 1)]\n",
    "\n",
    "    df = spark.createDataFrame(data, [\"id\", \"age\", \"salary\", \"department\"])\n",
    "    \n",
    "    # Add some randomness to make the data more interesting\n",
    "    df = df.withColumn(\"salary\", when(rand() > 0.7, df.salary * 1.5).otherwise(df.salary))\n",
    "    df = df.withColumn(\"age\", when(rand() > 0.8, lit(18)).otherwise(df.age))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the sample DataFrame\n",
    "sample_df = create_sample_dataframe(30)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Sample DataFrame:\")\n",
    "sample_df.show()\n",
    "\n",
    "# Save the DataFrame as a table\n",
    "table_name = \"employees2\"\n",
    "sample_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"DataFrame saved as table: {table_name}\")\n",
    "\n",
    "# To verify, you can read the table back\n",
    "df_from_table = spark.table(table_name)\n",
    "print(\"\\nDataFrame read from table:\")\n",
    "df_from_table.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f2a591-1f50-47bc-9c8c-78b819daffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Customer Data:\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "|customer_id|first_name|last_name|             email|phone_number|credit_score|account_balance|is_active|\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "|       1000|   ddnfzrv| idpegens|owyjlvlr@ihwyq.com|868-796-1569|         783|       99999.64|    false|\n",
      "|       1001|   hyjcdaj| imcxcrkb|negmrysm@zemrt.com|818-456-9245|         525|       69509.03|     true|\n",
      "|       1002|   zgkqbsl| nnlsrcde|uroefoph@ptnlp.com|725-990-6281|         687|       75089.69|     true|\n",
      "|       1003|   krztpaw| vwztywvh|fkgohydp@cjfdg.com|558-706-2373|         674|       91632.33|     true|\n",
      "|       1004|   qfznnsf| eubamudu|zwjlcmwf@khzgo.com|131-425-4309|         454|        9779.89|    false|\n",
      "|       1005|   kqlwccq| xztnoepz|nqhsrvhw@jnoay.com|674-846-3785|         497|       30723.59|     true|\n",
      "|       1006|   dittnez| mbrhgiuz|brkumtbb@kgsux.com|163-578-5563|         476|       73939.85|     true|\n",
      "|       1007|   aviuabl| twzfjknb|ddnxsxks@pyoep.com|363-808-4928|         398|       35150.28|     true|\n",
      "|       1008|   agcgcdc| chwadxaj|kjxvxzfu@jtmup.com|621-198-9823|         356|       94474.61|     true|\n",
      "|       1009|   smxpond| hrpnohii|zizavrww@ndhgq.com|907-606-1295|         517|       78859.16|    false|\n",
      "|       1010|   amifhwl| nirfprok|svcxfgdn@ziqdv.com|647-381-4841|         707|        4345.14|    false|\n",
      "|       1011|   hijjyso| byawnibb|qotrflhh@qejtf.com|730-516-4411|         440|       43169.65|     true|\n",
      "|       1012|   rfxmngp| yzzutnnn|gsctmdgj@vnshf.com|184-445-9737|         443|       21231.08|    false|\n",
      "|       1013|   gklfkeg| rfxzpano|mcilzkjm@aezpf.com|524-600-5176|         714|       47719.11|     true|\n",
      "|       1014|   twfqcbi| ohqeptdp|aynhvvfg@ledtt.com|146-726-8907|         644|        76878.4|     true|\n",
      "|       1015|   siinmqf| kycrmkbj|ichnjteg@rikbt.com|686-473-5006|         620|       50437.95|     true|\n",
      "|       1016|   dcfslsn| vuduzqqy|zhlpbhzu@fehdm.com|924-725-3307|         487|       89204.16|    false|\n",
      "|       1017|   spwlcbk| xwujswkg|zavzrpub@hvebs.com|319-630-2834|         366|       32546.37|    false|\n",
      "|       1018|   uhssexh| oqomjxvq|tysmzlmm@nlwvm.com|551-944-9633|         722|       75975.12|     true|\n",
      "|       1019|   tnnfvce| vpfyqegv|bnblixej@jfvlg.com|383-351-4020|         586|       47865.42|    false|\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "\n",
      "Data saved as table: customers\n",
      "\n",
      "Verifying saved data:\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "|customer_id|first_name|last_name|             email|phone_number|credit_score|account_balance|is_active|\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "|       1000|   ddnfzrv| idpegens|owyjlvlr@ihwyq.com|868-796-1569|         783|       99999.64|    false|\n",
      "|       1001|   hyjcdaj| imcxcrkb|negmrysm@zemrt.com|818-456-9245|         525|       69509.03|     true|\n",
      "|       1002|   zgkqbsl| nnlsrcde|uroefoph@ptnlp.com|725-990-6281|         687|       75089.69|     true|\n",
      "|       1003|   krztpaw| vwztywvh|fkgohydp@cjfdg.com|558-706-2373|         674|       91632.33|     true|\n",
      "|       1004|   qfznnsf| eubamudu|zwjlcmwf@khzgo.com|131-425-4309|         454|        9779.89|    false|\n",
      "|       1005|   kqlwccq| xztnoepz|nqhsrvhw@jnoay.com|674-846-3785|         497|       30723.59|     true|\n",
      "|       1006|   dittnez| mbrhgiuz|brkumtbb@kgsux.com|163-578-5563|         476|       73939.85|     true|\n",
      "|       1007|   aviuabl| twzfjknb|ddnxsxks@pyoep.com|363-808-4928|         398|       35150.28|     true|\n",
      "|       1008|   agcgcdc| chwadxaj|kjxvxzfu@jtmup.com|621-198-9823|         356|       94474.61|     true|\n",
      "|       1009|   smxpond| hrpnohii|zizavrww@ndhgq.com|907-606-1295|         517|       78859.16|    false|\n",
      "|       1010|   amifhwl| nirfprok|svcxfgdn@ziqdv.com|647-381-4841|         707|        4345.14|    false|\n",
      "|       1011|   hijjyso| byawnibb|qotrflhh@qejtf.com|730-516-4411|         440|       43169.65|     true|\n",
      "|       1012|   rfxmngp| yzzutnnn|gsctmdgj@vnshf.com|184-445-9737|         443|       21231.08|    false|\n",
      "|       1013|   gklfkeg| rfxzpano|mcilzkjm@aezpf.com|524-600-5176|         714|       47719.11|     true|\n",
      "|       1014|   twfqcbi| ohqeptdp|aynhvvfg@ledtt.com|146-726-8907|         644|        76878.4|     true|\n",
      "|       1015|   siinmqf| kycrmkbj|ichnjteg@rikbt.com|686-473-5006|         620|       50437.95|     true|\n",
      "|       1016|   dcfslsn| vuduzqqy|zhlpbhzu@fehdm.com|924-725-3307|         487|       89204.16|    false|\n",
      "|       1017|   spwlcbk| xwujswkg|zavzrpub@hvebs.com|319-630-2834|         366|       32546.37|    false|\n",
      "|       1018|   uhssexh| oqomjxvq|tysmzlmm@nlwvm.com|551-944-9633|         722|       75975.12|     true|\n",
      "|       1019|   tnnfvce| vpfyqegv|bnblixej@jfvlg.com|383-351-4020|         586|       47865.42|    false|\n",
      "+-----------+----------+---------+------------------+------------+------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_string(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "def generate_random_email():\n",
    "    username = generate_random_string(8)\n",
    "    domain = generate_random_string(5)\n",
    "    return f\"{username}@{domain}.com\"\n",
    "\n",
    "def generate_random_phone():\n",
    "    return f\"{random.randint(100, 999)}-{random.randint(100, 999)}-{random.randint(1000, 9999)}\"\n",
    "\n",
    "def generate_customer_data(num_rows):\n",
    "    data = []\n",
    "    for i in range(num_rows):\n",
    "        customer = (\n",
    "            i + 1000,  # customer_id\n",
    "            generate_random_string(7),  # first_name\n",
    "            generate_random_string(8),  # last_name\n",
    "            generate_random_email(),  # email\n",
    "            generate_random_phone(),  # phone_number\n",
    "            random.randint(300, 850),  # credit_score\n",
    "            round(random.uniform(0, 100000), 2),  # account_balance\n",
    "            random.choice([True, False])  # is_active\n",
    "        )\n",
    "        data.append(customer)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema for the customers table\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"phone_number\", StringType(), False),\n",
    "    StructField(\"credit_score\", IntegerType(), False),\n",
    "    StructField(\"account_balance\", DoubleType(), False),\n",
    "    StructField(\"is_active\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "# Generate the data\n",
    "num_rows = 20\n",
    "customer_data = generate_customer_data(num_rows)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "df_customers = spark.createDataFrame(customer_data, schema)\n",
    "\n",
    "# Show the generated data\n",
    "print(\"Generated Customer Data:\")\n",
    "df_customers.show()\n",
    "\n",
    "# Save as a table\n",
    "table_name = \"customers\"\n",
    "df_customers.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "print(f\"Data saved as table: {table_name}\")\n",
    "\n",
    "# Verify the saved data\n",
    "print(\"\\nVerifying saved data:\")\n",
    "spark.table(table_name).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fa0956-64ca-487f-8170-6f90a587fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, lit, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from typing import List, Dict\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "class Rule:\n",
    "    def __init__(self, name: str, condition: str):\n",
    "        self.name = name\n",
    "        self.condition = condition\n",
    "\n",
    "class RulesEngine:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.metrics_schema = StructType([\n",
    "            StructField(\"table_name\", StringType(), False),\n",
    "            StructField(\"rule_name\", StringType(), False),\n",
    "            StructField(\"passed\", LongType(), False),\n",
    "            StructField(\"failed\", LongType(), False),\n",
    "            StructField(\"total\", LongType(), False),\n",
    "            StructField(\"timestamp\", TimestampType(), False)\n",
    "        ])\n",
    "        self.metrics_df = spark.createDataFrame([], schema=self.metrics_schema)\n",
    "\n",
    "    def apply_rule(self, df, rule: Rule, table_name: str):\n",
    "        result = df.withColumn(rule.name, expr(rule.condition))\n",
    "        passed = result.filter(expr(f\"{rule.name} == true\")).count()\n",
    "        failed = result.filter(expr(f\"{rule.name} == false\")).count()\n",
    "        total = passed + failed\n",
    "\n",
    "        metrics = {\n",
    "            \"table_name\": table_name,\n",
    "            \"rule_name\": rule.name,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"total\": total,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "\n",
    "        new_row = self.spark.createDataFrame([metrics], schema=self.metrics_schema)\n",
    "        self.metrics_df = self.metrics_df.union(new_row)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def run_rules(self, table_name: str, rules: List[Rule], filter_condition: str = None, num_partitions: int = None):\n",
    "        df = self.spark.table(table_name)\n",
    "        if filter_condition:\n",
    "            df = df.filter(expr(filter_condition))\n",
    "        if num_partitions:\n",
    "            df = df.repartition(num_partitions)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.apply_rule, df, rule, table_name) for rule in rules]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "        return self.spark.createDataFrame(results)\n",
    "\n",
    "    def save_metrics(self, path: str):\n",
    "        self.metrics_df.write.format(\"parquet\").mode(\"append\").save(path)\n",
    "\n",
    "def run_rules_for_tables(spark: SparkSession, tables_config: List[Dict], metrics_path: str):\n",
    "    engine = RulesEngine(spark)\n",
    "\n",
    "    for config in tables_config:\n",
    "        table_name = config['table_name']\n",
    "        rules = [Rule(name=rule['name'], condition=rule['condition']) for rule in config['rules']]\n",
    "        filter_condition = config.get('filter_condition')\n",
    "        num_partitions = config.get('num_partitions')\n",
    "\n",
    "        print(f\"Running rules for table: {table_name}\")\n",
    "        results = engine.run_rules(table_name, rules, filter_condition, num_partitions)\n",
    "        results.show()\n",
    "\n",
    "    print(\"Saving metrics to Delta table\")\n",
    "    engine.save_metrics(metrics_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcb13f4-b72d-4243-82f7-f84f1324bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rules for table: employees2\n",
      "+------+------+----------------+----------+--------------------+-----+\n",
      "|failed|passed|       rule_name|table_name|           timestamp|total|\n",
      "+------+------+----------------+----------+--------------------+-----+\n",
      "|     0|    22|       age_check|employees2|2024-06-22 13:34:...|   22|\n",
      "|    19|     3|department_check|employees2|2024-06-22 13:34:...|   22|\n",
      "|    18|     4|   complex_check|employees2|2024-06-22 13:34:...|   22|\n",
      "|     5|    17|    salary_check|employees2|2024-06-22 13:34:...|   22|\n",
      "+------+------+----------------+----------+--------------------+-----+\n",
      "\n",
      "Running rules for table: customers\n",
      "+------+------+------------------+----------+--------------------+-----+\n",
      "|failed|passed|         rule_name|table_name|           timestamp|total|\n",
      "+------+------+------------------+----------+--------------------+-----+\n",
      "|     0|    20|     balance_check| customers|2024-06-22 13:34:...|   20|\n",
      "|    16|     4|credit_score_check| customers|2024-06-22 13:34:...|   20|\n",
      "|     8|    12|      active_check| customers|2024-06-22 13:34:...|   20|\n",
      "+------+------+------------------+----------+--------------------+-----+\n",
      "\n",
      "Saving metrics to Delta table\n"
     ]
    }
   ],
   "source": [
    "# Define rules\n",
    "tables_config = [\n",
    "    {\n",
    "        \"table_name\": \"employees2\",\n",
    "        \"rules\": [\n",
    "            {\"name\": \"age_check\", \"condition\": \"age >= 18\"},\n",
    "            {\"name\": \"salary_check\", \"condition\": \"salary > 50000\"},\n",
    "            {\"name\": \"department_check\", \"condition\": \"department == 'IT'\"},\n",
    "            {\"name\": \"complex_check\", \"condition\": \"age > 30 and salary > 75000\"}\n",
    "        ],\n",
    "        \"filter_condition\": \"age > 25\",\n",
    "        \"num_partitions\": 2\n",
    "    },\n",
    "    {\n",
    "        \"table_name\": \"customers\",\n",
    "        \"rules\": [\n",
    "            {\"name\": \"credit_score_check\", \"condition\": \"credit_score > 700\"},\n",
    "            {\"name\": \"balance_check\", \"condition\": \"account_balance > 1000\"},\n",
    "            {\"name\": \"active_check\", \"condition\": \"is_active == true\"}\n",
    "        ],\n",
    "        \"num_partitions\": 4\n",
    "    }\n",
    "    # Add more table configurations as needed\n",
    "]\n",
    "\n",
    "metrics_path = \"./metrics_delta_table\"\n",
    "\n",
    "run_rules_for_tables(spark, tables_config, metrics_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02295f3-1723-41e2-abd8-a09e0ded9821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
