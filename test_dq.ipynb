{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data setup\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "import yaml\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataValidationTest\").getOrCreate()\n",
    "\n",
    "# Define schema for the sales table\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"order_date\", DateType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"revenue\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Generate test data\n",
    "def generate_test_data():\n",
    "    data = []\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\"]\n",
    "    start_date = date(2024, 1, 1)\n",
    "    \n",
    "    for i in range(1000):  # Generate 1000 sample records\n",
    "        order_date = start_date + timedelta(days=i % 365)\n",
    "        category = categories[i % len(categories)]\n",
    "        quantity = (i % 10) + 1\n",
    "        revenue = quantity * (100.00 + (i % 100))\n",
    "        \n",
    "        data.append((f\"ORD-{i:04d}\", order_date, category, quantity, revenue))\n",
    "    \n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create and register the test table\n",
    "test_df = generate_test_data()\n",
    "test_df.createOrReplaceTempView(\"sales_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------+-------+\n",
      "|order_id|order_date|product_category|quantity|revenue|\n",
      "+--------+----------+----------------+--------+-------+\n",
      "|ORD-0000|2024-01-01|Electronics     |1       |100.0  |\n",
      "|ORD-0001|2024-01-02|Clothing        |2       |202.0  |\n",
      "|ORD-0002|2024-01-03|Books           |3       |306.0  |\n",
      "|ORD-0003|2024-01-04|Home            |4       |412.0  |\n",
      "|ORD-0004|2024-01-05|Electronics     |5       |520.0  |\n",
      "|ORD-0005|2024-01-06|Clothing        |6       |630.0  |\n",
      "|ORD-0006|2024-01-07|Books           |7       |742.0  |\n",
      "|ORD-0007|2024-01-08|Home            |8       |856.0  |\n",
      "|ORD-0008|2024-01-09|Electronics     |9       |972.0  |\n",
      "|ORD-0009|2024-01-10|Clothing        |10      |1090.0 |\n",
      "+--------+----------+----------------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sales_table\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/sthitaprajnasahoo/local_learning/sparkdq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def execute_sql_rule(spark, sql_expression, variables):\n",
    "    formatted_sql = sql_expression.format(**variables)\n",
    "    print(formatted_sql)\n",
    "    df = spark.sql(formatted_sql)\n",
    "    print(df.show())\n",
    "    return df\n",
    "\n",
    "def compare_result(result_df, expected_result):\n",
    "    column = expected_result['column']\n",
    "    operator = expected_result['operator']\n",
    "    value = expected_result['value']\n",
    "\n",
    "    if operator == '>':\n",
    "        return result_df.select(col(column) > value).first()[0]\n",
    "    elif operator == '<':\n",
    "        return result_df.select(col(column) < value).first()[0]\n",
    "    elif operator == 'between':\n",
    "        return result_df.select((col(column) >= value[0]) & (col(column) <= value[1])).first()[0]\n",
    "    # Add more operators as needed\n",
    "\n",
    "def validate_rules(spark, config, variables):\n",
    "    results = []\n",
    "    for table in config['tables']:\n",
    "        for rule in table['rules']:\n",
    "            result_df = execute_sql_rule(spark, rule['sql_expression'], variables)\n",
    "            is_valid = compare_result(result_df, rule['expected_result'])\n",
    "            \n",
    "            results.append({\n",
    "                'table_name': table['name'],\n",
    "                'rule_id': rule['rule_id'],\n",
    "                'sql_expression': rule['sql_expression'],\n",
    "                'expected_result': rule['expected_result'],\n",
    "                'is_valid': is_valid\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def store_results(results):\n",
    "    results_df = spark.createDataFrame(results)\n",
    "    results_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/validation_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config/validation_rules.yaml')\n",
    "variables = {\n",
    "    \"start_date\": \"2024-01-01\",\n",
    "    \"categories\": \"('Electronics', 'Clothing')\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tables': [{'name': 'sales_table',\n",
       "   'rules': [{'rule_id': 'rule1',\n",
       "     'sql_expression': \"SELECT MAX(revenue) as max_revenue FROM sales_table WHERE order_date > '{start_date}'\",\n",
       "     'expected_result': {'column': 'max_revenue',\n",
       "      'operator': '>',\n",
       "      'value': 10000}},\n",
       "    {'rule_id': 'rule2',\n",
       "     'sql_expression': 'SELECT AVG(quantity) as avg_quantity FROM sales_table WHERE product_category IN {categories}',\n",
       "     'expected_result': {'column': 'avg_quantity',\n",
       "      'operator': 'between',\n",
       "      'value': [5, 100]}}]}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT MAX(revenue) as max_revenue FROM sales_table WHERE order_date > '2024-01-01'\n",
      "+-----------+\n",
      "|max_revenue|\n",
      "+-----------+\n",
      "|     1990.0|\n",
      "+-----------+\n",
      "\n",
      "None\n",
      "SELECT AVG(quantity) as avg_quantity FROM sales_table WHERE product_category IN ('Electronics', 'Clothing')\n",
      "+------------+\n",
      "|avg_quantity|\n",
      "+------------+\n",
      "|         5.5|\n",
      "+------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "validation_results = validate_rules(spark, config, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_name': 'sales_table',\n",
       "  'rule_id': 'rule1',\n",
       "  'sql_expression': \"SELECT MAX(revenue) as max_revenue FROM sales_table WHERE order_date > '{start_date}'\",\n",
       "  'expected_result': {'column': 'max_revenue',\n",
       "   'operator': '>',\n",
       "   'value': 10000},\n",
       "  'is_valid': False},\n",
       " {'table_name': 'sales_table',\n",
       "  'rule_id': 'rule2',\n",
       "  'sql_expression': 'SELECT AVG(quantity) as avg_quantity FROM sales_table WHERE product_category IN {categories}',\n",
       "  'expected_result': {'column': 'avg_quantity',\n",
       "   'operator': 'between',\n",
       "   'value': [5, 100]},\n",
       "  'is_valid': True}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkdq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
